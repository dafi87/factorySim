env: '' #Gets populated on runtime
callbacks: '' #Gets populated on runtime
env_config:
  inputfile: '' #Gets populated on runtime
  obs_type: image
  Loglevel: 0
  width: 84
  heigth: 84
  maxMF_Elements: 3
  outputScale: 10
  rendermode: null
  factoryconfig: SMALL
model:
  custom_model: my_model
  dim: 256
  # Change individual keys in that dict by overriding them, e.g.
  conv_filters: null
  conv_activation: relu
  # == Attention Nets (experimental: torch-version is untested) ==
  # Whether to use a GTrXL ("Gru transformer XL"; attention net) as the
  # wrapper Model around the default Model.
  use_attention: false
  # The number of transformer units within GTrXL.
  # A transformer unit in GTrXL consists of a) MultiHeadAttention module and
  # b) a position-wise MLP.
  attention_num_transformer_units: 1
  # The input and output size of each transformer unit.
  attention_dim: 64
  # The number of attention heads within the MultiHeadAttention units.
  attention_num_heads: 1
  # The dim of a single head (within the MultiHeadAttention units).
  attention_head_dim: 32
  # The memory sizes for inference and training.
  attention_memory_inference: 50
  attention_memory_training: 50
  # The output dim of the position-wise MLP.
  attention_position_wise_mlp_dim: 32
  # The initial bias values for the 2 GRU gates within a transformer unit.
  attention_init_gru_gate_bias: 2.0
  # Whether to feed a_{t-n:t-1} to GTrXL (one-hot encoded if discrete).
  attention_use_n_prev_actions: 0
  # Whether to feed r_{t-n:t-1} to GTrXL.
  attention_use_n_prev_rewards: 0


# Evaluate once per training iteration.
evaluation_interval: 10
# Run evaluation on (at least) ten episodes
evaluation_duration: 10
# ... using one evaluation worker (setting this to 0 will cause
# evaluation to run on the local evaluation worker, blocking
# training until evaluation is done).
evaluation_num_workers: 1

# Special evaluation config. Keys specified here will override
# the same keys in the main config, but only for evaluation.
evaluation_config:
# Store videos in this relative directory here inside
# the default output dir (~/ray_results/...).
# Alternatively, you can specify an absolute path.
# Set to True for using the default output dir (~/ray_results/...).
# Set to False for not recording anything.
#"record_env": os.path.join(os.path.dirname(os.path.realpath(__file__)), "Output"),
# Render the env while evaluating.
# Note that this will always only render the 1st RolloutWorker's
# env and only the 1st sub-env in a vectorized env.
  render_env: true
  env_config:
    rendermode: human


render_env: false
#"tf", "tf2", "tfe", "torch"
framework: tf2
eager_tracing: true
# Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.
num_gpus: 1  #int(os.environ.get("RLLIB_NUM_GPUS", "0"))
num_workers: 20  # parallelism  #12
num_envs_per_worker: 1

# Should be one of DEBUG, INFO, WARN, or ERROR
log_level: ERROR
# Whether - upon a worker failure - RLlib will try to recreate the lost worker as
# an identical copy of the failed one. The new worker will only differ from the
# failed one in its `self.recreated_worker=True` property value. It will have
# the same `worker_index` as the original one.
# If True, the `ignore_worker_failures` setting will be ignored.
recreate_failed_workers: true
restart_failed_sub_environments: true

rollout_fragment_length: 200
train_batch_size:  3000
sgd_minibatch_size: 100
num_sgd_iter: 4
gamma: 0.99
# The default learning rate.
lr: 0.00025  #0.00002,
lambda: 0.95
vf_loss_coeff: 0.5
entropy_coeff: 0.01
clip_param: 0.2
vf_clip_param: 10.0
kl_target: 0.01
